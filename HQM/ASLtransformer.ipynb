{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "29799daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a9bac254",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkEmbedding(nn.Module):\n",
    "    def __init__(self,ins,outs,name,hidden_size=50):\n",
    "        super(LandmarkEmbedding, self).__init__()\n",
    "        self.outs = outs\n",
    "        self.name = name\n",
    "        self.fc1 = nn.Linear(ins,hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size,outs)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        hidden = (self.fc1(x))\n",
    "        output = self.fc2(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "58061c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10, 20])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    hand_embedding = LandmarkEmbedding(21*2, 20, name = 'hand')\n",
    "# (batch_size, ncols*ndims)\n",
    "    hand = torch.randn(20, 10, 21*2)\n",
    "    print(hand_embedding(hand).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "175563da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lips_idxs = range(0,40)\n",
    "hand_idxs = range(40,61)\n",
    "pose_idxs = range(61,66)\n",
    "ndims = 2\n",
    "input_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d5f018f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self,units,hidden_size=50):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.units = units\n",
    "        self.lips_embedding = LandmarkEmbedding(len(lips_idxs)*ndims,units,'lips')\n",
    "        self.hand_embedding = LandmarkEmbedding(len(hand_idxs)*ndims,units,'hand')\n",
    "        self.pose_embedding = LandmarkEmbedding(len(pose_idxs)*ndims,units,'pose')\n",
    "        self.S = nn.Parameter(torch.randn(3))        \n",
    "        self.fc = nn.Linear(384,384)\n",
    "        self.pe = nn.Embedding(input_size+1, 384)\n",
    "        \n",
    "    def forward(self, x, non_empty_frame_idxs):\n",
    "# (batch_size, input_size, ncols, ndims)\n",
    "        batch_size,nframes,ncols,ndims = x.shape\n",
    "        lips = torch.reshape(x[:,:,lips_idxs,:],(batch_size,nframes,len(lips_idxs)*ndims))\n",
    "        hand = torch.reshape(x[:,:,hand_idxs,:],(batch_size,nframes,len(hand_idxs)*ndims))\n",
    "        pose = torch.reshape(x[:,:,pose_idxs,:],(batch_size,nframes,len(pose_idxs)*ndims))\n",
    "        print(lips.shape)\n",
    "        lips_embedding = self.lips_embedding(lips)\n",
    "        print(lips_embedding.shape)\n",
    "        hand_embedding = self.hand_embedding(hand)\n",
    "        pose_embedding = self.pose_embedding(pose)\n",
    "        print(lips_embedding.shape)\n",
    "        S = torch.softmax(self.S, dim=0)\n",
    "        full_embedding = S[0]*lips_embedding + S[1]*hand_embedding + S[2]*pose_embedding\n",
    "        print(full_embedding.shape)\n",
    "        max_non_empty_frame_idxs = torch.max(non_empty_frame_idxs,dim=1,keepdim=True)[0]\n",
    "        print(max_non_empty_frame_idxs.shape)\n",
    "        max_non_empty_frame_idxs = torch.clamp(max_non_empty_frame_idxs, min=1, max=float('inf'))\n",
    "        print(max_non_empty_frame_idxs.shape)\n",
    "        normalised_non_empty_frame_idxs = torch.where(non_empty_frame_idxs<0,\n",
    "                                                      torch.tensor(input_size),\n",
    "                                                      non_empty_frame_idxs/max_non_empty_frame_idxs * input_size).int()\n",
    "        print(normalised_non_empty_frame_idxs.shape)\n",
    "        output = self.fc(full_embedding) + self.pe(normalised_non_empty_frame_idxs)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "da9ec76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10, 80])\n",
      "torch.Size([20, 10, 384])\n",
      "torch.Size([20, 10, 384])\n",
      "torch.Size([20, 10, 384])\n",
      "torch.Size([20, 1])\n",
      "torch.Size([20, 1])\n",
      "torch.Size([20, 10])\n",
      "torch.Size([20, 10, 384])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    embedding = Embedding(384)\n",
    "    inputs = torch.randn(20, 10, 66, 2)\n",
    "    idxs =torch.randint(low=0, high=10, size=(20, 10))\n",
    "    print(embedding(inputs,idxs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a56b23ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,n_blocks):\n",
    "        super(Transformer,self).__init__()\n",
    "        self.n_blocks = n_blocks\n",
    "        self.mahs = [nn.MultiheadAttention(384, num_heads=2,batch_first=True) for _ in range(n_blocks)]\n",
    "        self.mlps = [nn.Sequential(nn.Linear(384,20),nn.ReLU(),nn.Linear(20,384)) for _ in range(n_blocks)]\n",
    "        self.embedding = Embedding(384)\n",
    "        self.classifier = nn.Sequential(nn.Linear(384,250),nn.Softmax(dim=-1))\n",
    "        \n",
    "    def forward(self, x, padding_mask):\n",
    "        print('padding_mask: ',padding_mask.shape) \n",
    "        inputs = self.embedding(x, padding_mask)\n",
    "        print('inputs: ',inputs.shape) \n",
    "        for i in range(self.n_blocks):\n",
    "            inputs = inputs + self.mahs[i](inputs,inputs,inputs,key_padding_mask=padding_mask)[0]\n",
    "            inputs = inputs + self.mlps[i](inputs)\n",
    "        print('inputs: ',inputs.shape)    \n",
    "        mask = padding_mask.unsqueeze(2)\n",
    "        output = torch.sum(inputs * mask, axis=1) / torch.sum(mask, axis=1)\n",
    "        print('output: ',output.shape) \n",
    "        return self.classifier(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dda3502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1bff0a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn: torch.Size([20, 10])\n"
     ]
    }
   ],
   "source": [
    "#(batch_size,input_size,ncols,ndims)\n",
    "attn_mask = torch.randint(0, 10, (20,10))\n",
    "print('attn:',attn_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2afd6959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding_mask:  torch.Size([20, 10])\n",
      "torch.Size([20, 10, 80])\n",
      "torch.Size([20, 10, 384])\n",
      "torch.Size([20, 10, 384])\n",
      "torch.Size([20, 10, 384])\n",
      "torch.Size([20, 1])\n",
      "torch.Size([20, 1])\n",
      "torch.Size([20, 10])\n",
      "inputs:  torch.Size([20, 10, 384])\n",
      "inputs:  torch.Size([20, 10, 384])\n",
      "output:  torch.Size([20, 384])\n",
      "torch.Size([20, 250])\n"
     ]
    }
   ],
   "source": [
    "print(model(torch.randn(20, 10, 66, 2),attn_mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7647365a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792bb3ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63abadd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
